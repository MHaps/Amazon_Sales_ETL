{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column renamed and file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "#Rename\n",
    "import pandas as pd\n",
    "df = pd.read_csv('amazon_sales_report_original.csv')\n",
    "\n",
    "df.rename(columns={'Fulfilment': 'fulfilment'}, inplace=True)\n",
    "\n",
    "df.to_csv('amazon_sales_report_original.csv', index=False)\n",
    "print(\"Column renamed and file saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['order_status', 'fulfillment', 'sales_channel', 'ship_service_level', 'courier_status', 'size', 'asin'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Program Files\\PostgreSQL\\16\\data\\python_files\\zpre_processing.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Program%20Files/PostgreSQL/16/data/python_files/zpre_processing.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Program%20Files/PostgreSQL/16/data/python_files/zpre_processing.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mamazon_sales_report_original.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Program%20Files/PostgreSQL/16/data/python_files/zpre_processing.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df_category \u001b[39m=\u001b[39m df[[\u001b[39m'\u001b[39;49m\u001b[39mquantity\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mamount\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39morder_id\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39morder_status\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mfulfillment\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msales_channel\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mship_service_level\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Program%20Files/PostgreSQL/16/data/python_files/zpre_processing.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                   \u001b[39m'\u001b[39;49m\u001b[39mcourier_status\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdate\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcategory\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39msize\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39masin\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mship_city\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mship_state\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mship_postal_code\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mpromotion_id\u001b[39;49m\u001b[39m'\u001b[39;49m]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Program%20Files/PostgreSQL/16/data/python_files/zpre_processing.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m output_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mC:/Program Files/PostgreSQL/16/data/python_files/all_tables`.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Program%20Files/PostgreSQL/16/data/python_files/zpre_processing.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m df_category\u001b[39m.\u001b[39mto_csv(output_file, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Haber\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3812\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3813\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3815\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Haber\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6067\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6070\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6072\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   6073\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6074\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Haber\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6133\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6132\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 6133\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['order_status', 'fulfillment', 'sales_channel', 'ship_service_level', 'courier_status', 'size', 'asin'] not in index\""
     ]
    }
   ],
   "source": [
    "#Extract\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('amazon_sales_report_original.csv')\n",
    "df_category = df[['quantity','amount', 'order_id', 'order_status', 'fulfilment', 'sales_channel', 'ship_service_level',\n",
    "                  'courier_status', 'date', 'category', 'size', 'asin', 'ship_city', 'ship_state', 'ship_postal_code', 'promotion_id']]\n",
    "\n",
    "output_file = 'C:/Program Files/PostgreSQL/16/data/python_files/all_tables`.csv'\n",
    "df_category.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count\n",
    "import pandas as pd\n",
    "df = pd.read_csv('z_amount_extracted.csv')\n",
    "print(df['amount'].count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count Rows/Cols:\n",
    "df = pd.read_csv('')\n",
    "\n",
    "#num_rows = df.shape[0]\n",
    "\n",
    "num_rows = df['amount'].count()\n",
    "\n",
    "print(\"Number of rows:\", num_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Rows\n",
    "import pandas as pd\n",
    "\n",
    "original_csv_path = 'amazon_sales_report_original.csv'\n",
    "new_rows_csv_path = 'amazon_sales_report_16k.csv'\n",
    "\n",
    "df_14k = pd.read_csv(new_rows_csv_path)\n",
    "\n",
    "df_original = pd.read_csv(original_csv_path, nrows=2000, skiprows=15000)\n",
    "\n",
    "combined_df = pd.concat([df_14k, df_original], ignore_index=True)\n",
    "\n",
    "combined_df.to_csv(new_rows_csv_path, index=False)\n",
    "\n",
    "print(\"2,000 rows added to amazon_sales_report_14k.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column Names\n",
    "import pandas as pd\n",
    "original_csv_path = 'amazon_sales_report_original.csv'\n",
    "existing_csv_path = 'amazon_sales_report16k.csv'\n",
    "\n",
    "#df_existing = pd.read_csv(existing_csv_path)\n",
    "\n",
    "#df_original = pd.read_csv(original_csv_path, usecols=['categoryname'], nrows=2000, skiprows=14000)\n",
    "\n",
    "#print(df_original)\n",
    "#combined_df = pd.concat([df_existing, df_original], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame back to the existing 16k rows CSV file\n",
    "#combined_df.to_csv(existing_csv_path, index=False)\n",
    "\n",
    "#print(\"2,000 rows added to amazon_sales_report_16k.csv for the 'categoryname' column\")\n",
    "\n",
    "header = pd.read_csv(existing_csv_path, nrows=1)\n",
    "\n",
    "print(header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identical Cols\n",
    "original_csv_path = 'amazon_sales_report_original.csv'\n",
    "new_csv_path = 'amazon_sales_report_16k.csv'\n",
    "\n",
    "#df_original = pd.read_csv(original_csv_path, nrows=16000)\n",
    "#df_new = pd.read_csv(new_csv_path, nrows=16000)\n",
    "\n",
    "\n",
    "original_columns = pd.read_csv(original_csv_path, nrows=0).columns\n",
    "new_columns = pd.read_csv(new_csv_path, nrows=0).columns\n",
    "\n",
    "are_columns_identical = original_columns.equals(new_columns)\n",
    "\n",
    "if are_columns_identical:\n",
    "    print(\"The column names in the original and new CSV files are identical.\")\n",
    "else:\n",
    "    print(\"The column names in the original and new CSV files are not identical.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file\n",
    "csv_file_path = 'amazon_sales_report_16k.csv'\n",
    "\n",
    "# Read the first 16k rows from the CSV file\n",
    "df = pd.read_csv(csv_file_path, nrows=16000)\n",
    "\n",
    "# Delete all rows after the 16k row\n",
    "df = df.iloc[:16000]  # Keep the first 16k rows\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "new_csv_path = 'amazon_sales_report_16k.csv'\n",
    "df.to_csv(new_csv_path, index=False)\n",
    "\n",
    "print(\"Rows after the 16k row deleted and saved to amazon_sales_report_up_to_16k.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace Row Value:\n",
    "import pandas as pd\n",
    "df = pd.read_csv('db_dimension_promotion1.csv', skipinitialspace=True)\n",
    "\n",
    "#df.replace('', None, inplace=True)\n",
    "#df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "null_promotion_id_rows = df[df['promotion_id'].isnull()]\n",
    "#df.to_csv('db_dimension_promotion1.csv', index=False)\n",
    "\n",
    "print(null_promotion_id_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Data type\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('table_date_modified.csv')\n",
    "\n",
    "column_name = 'date'\n",
    "\n",
    "column_dtype = df[column_name].dtype\n",
    "print(f\"Datatype of '{column_name}': {column_dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique Duplicate rows Count\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "csv_file_path = 'db_dimension_promotion1.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Find duplicate rows based on the 'promotion_id' column\n",
    "duplicate_rows = df[df.duplicated(subset=['promotion_id'], keep=False)]\n",
    "\n",
    "# Keep only the first occurrence of each unique duplicate row\n",
    "unique_duplicate_rows = duplicate_rows.drop_duplicates(subset=['promotion_id'])\n",
    "\n",
    "# Get the count of unique duplicate rows\n",
    "duplicate_count = len(unique_duplicate_rows)\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    print(f'Unique Duplicate rows count: {duplicate_count}')\n",
    "    print('Unique Duplicate rows:')\n",
    "    print(unique_duplicate_rows)\n",
    "else:\n",
    "    print('No unique duplicate rows found.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('amazon_sales_report.csv')\n",
    "selected_columns = ['order_id', 'date', 'status', 'fulfillment', 'sales_channel', 'ship_service_level', 'courier_status']\n",
    "new_df = df[selected_columns]\n",
    "\n",
    "new_df.to_csv('db_dimension_orders.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value Counts\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('db_dimension_promotion.csv')\n",
    "\n",
    "# Count unique ship countries and their respective names\n",
    "unique_currency = df['promotion_id'].value_counts()\n",
    "\n",
    "# Print the results\n",
    "for currency, count in unique_currency.items():\n",
    "    print(f\"Promotion: {currency}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To count unique rows in the DataFrame:\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('z_currency_extracted.csv')\n",
    "\n",
    "unique_rows= df.drop_duplicates()\n",
    "print(\"unique rows:\", unique_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging CSVs\n",
    "import pandas as pd\n",
    "\n",
    "order_id_file = 'orders_date_product.csv'\n",
    "df_order_id = pd.read_csv(order_id_file)\n",
    "\n",
    "dimension_orders_csv_file = 'table_.csv'\n",
    "df_amount = pd.read_csv(dimension_orders_csv_file)\n",
    "\n",
    "merged_df = pd.concat([df_order_id, df_amount], axis=1)\n",
    "\n",
    "output_csv_file = 'orders_date_product.csv'\n",
    "\n",
    "merged_df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "print(\"Merged CSV file created:\", output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#True values count\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('z_b2b_extracted.csv')\n",
    "\n",
    "# Replace 'your_boolean_column' with the actual column name containing boolean values\n",
    "boolean_column = 'b2b'\n",
    "\n",
    "# Count the number of True values in the boolean column\n",
    "true_count = df[df[boolean_column] == True][boolean_column].count()\n",
    "\n",
    "print(f\"Count of True values in '{boolean_column}': {true_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add -n to Duplicates\n",
    "import collections\n",
    "import csv\n",
    "\n",
    "\n",
    "# Create a dictionary to track the count of each encountered order_id\n",
    "encountered_order_ids = collections.defaultdict(int)\n",
    "\n",
    "# Create a list to store the modified data\n",
    "modified_data = []\n",
    "\n",
    "with open('C:/Program Files/PostgreSQL/16/data/python_files/table_orders.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        order_id, status = line.strip().split(',')\n",
    "        \n",
    "        # Check if this order_id has been encountered before\n",
    "        if encountered_order_ids[order_id] > 0:\n",
    "            # Append the count as a suffix to the order_id\n",
    "            new_order_id = f\"{order_id}-{encountered_order_ids[order_id]}\"\n",
    "        else:\n",
    "            # This is the first occurrence, so use the original order_id\n",
    "            new_order_id = order_id\n",
    "        \n",
    "        # Increment the count for this order_id\n",
    "        encountered_order_ids[order_id] += 1\n",
    "        \n",
    "        # Use the new_order_id for your update operation\n",
    "        \n",
    "        # Append the modified data to the list\n",
    "        modified_data.append([new_order_id, status])\n",
    "\n",
    "# Close the cursor and connection (as you've done)\n",
    "\n",
    "# Save the modified data to a new CSV file\n",
    "with open('new_modified_data.csv', 'w', newline='') as new_csv_file:\n",
    "    csv_writer = csv.writer(new_csv_file)\n",
    "\n",
    "\n",
    "\n",
    "    csv_writer.writerow(['order_id', 'status'])  # Write the header\n",
    "    \n",
    "    \n",
    "    \n",
    "    csv_writer.writerows(modified_data)  # Write the modified data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Suffix\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def process_csv(input_file, output_file):\n",
    "    encountered_order_ids = {}\n",
    "    modified_data = []\n",
    "\n",
    "    with open(input_file, 'r', newline='') as f:\n",
    "        csv_reader = csv.DictReader(f)\n",
    "        \n",
    "        for row in csv_reader:\n",
    "            order_id = row.get('order_id')\n",
    "            status = row.get('status')\n",
    "\n",
    "            if order_id in encountered_order_ids:\n",
    "                encountered_order_ids[order_id] += 1\n",
    "                new_order_id = f\"{order_id}-{encountered_order_ids[order_id]}\"\n",
    "            else:\n",
    "                encountered_order_ids[order_id] = 0\n",
    "                new_order_id = order_id\n",
    "\n",
    "            modified_data.append({'order_id': new_order_id, 'status': status})\n",
    "\n",
    "    # Save the modified data to a new CSV file\n",
    "    with open(output_file, 'w', newline='') as new_csv_file:\n",
    "        fieldnames = ['order_id', 'status']\n",
    "        csv_writer = csv.DictWriter(new_csv_file, fieldnames=fieldnames)\n",
    "\n",
    "        csv_writer.writeheader()\n",
    "        csv_writer.writerows(modified_data)\n",
    "\n",
    "input_path = 'C:/Program Files/PostgreSQL/16/data/python_files/table_orders.csv'\n",
    "output_path = 'table_orders1.csv'\n",
    "\n",
    "if os.path.isfile(input_path):\n",
    "    process_csv(input_path, output_path)\n",
    "    print(\"Processing complete. Check the 'new_modified_data.csv' file.\")\n",
    "else:\n",
    "    print(\"Input file not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty rows with None\n",
    "with open('db_dimension_promotion.csv', 'r') as infile:\n",
    "    with open('db_dimension_promotion1.csv', 'w') as outfile:\n",
    "        for line in infile:\n",
    "            if line.strip() == '':\n",
    "                outfile.write('None\\n')  # Write 'None' for empty rows\n",
    "            else:\n",
    "                outfile.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '' with unknown\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('db_dimension_promotion.csv', skipinitialspace=True)\n",
    "\n",
    "df.replace({'': None, np.nan: None, ' ': None}, inplace=True)\n",
    "\n",
    "# Strip leading and trailing spaces from all string columns\n",
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "# Fill remaining None values with \"unknown\"\n",
    "df.fillna(\"unknown\", inplace=True)\n",
    "\n",
    "# Filter rows with null 'promotion_id' values\n",
    "null_promotion_id_rows = df[df['promotion_id'].isnull()]\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv('db_dimension_promotion1.csv', index=False)\n",
    "\n",
    "print(null_promotion_id_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add suffix to dupes\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('db_dimension_promotion1.csv')\n",
    "\n",
    "column_name = \"promotion_id\"\n",
    "\n",
    "suffix_dict = {}\n",
    "\n",
    "def assign_suffix(row):\n",
    "    value = row[column_name]\n",
    "    if value in suffix_dict:\n",
    "        suffix_dict[value] += 1\n",
    "        return f\"{value}+{suffix_dict[value]}\"\n",
    "    else:\n",
    "        suffix_dict[value] = 0\n",
    "        return value\n",
    "\n",
    "df[column_name] = df.apply(assign_suffix, axis=1)\n",
    "\n",
    "df.to_csv('unique_db_dimension_promotion.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Columns\n",
    "import pandas as pd\n",
    "df = pd.read_csv('amazon_sales_report_original.csv')\n",
    "\n",
    "cols_to_drop = ['Unnamed: 22']\n",
    "\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "df.to_csv('amazon_sales_report_original.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = 'C:/Program Files/PostgreSQL/16/data/python_files/table_date.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert the 'date' column from 'MM-DD-YY' to 'YYYY-MM-DD'\n",
    "df['date'] = pd.to_datetime(df['date'], format='%m-%d-%y').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Write the modified DataFrame back to a new CSV file\n",
    "new_file_path = 'C:/Program Files/PostgreSQL/16/data/python_files/table_date_modified.csv'\n",
    "df.to_csv(new_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element 'date' contains non-whitespace characters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_data = pd.read_csv('table_date_modified.csv')\n",
    "\n",
    "for entry in csv_data:\n",
    "    if entry.isspace():\n",
    "        print(f\"Element '{entry}' is whitespace\")\n",
    "    elif not entry.strip():\n",
    "        print(f\"Element '{entry}' is empty or contains only whitespace\")\n",
    "    else:\n",
    "        print(f\"Element '{entry}' contains non-whitespace characters\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
